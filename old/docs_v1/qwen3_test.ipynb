{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7093d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 22:00:26 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: Não foi possível encontrar o módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Entrez\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# --- CONFIGURATION ---\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Required by NCBI to use the Entrez API\u001b[39;00m\n\u001b[32m     13\u001b[39m ENTREZ_EMAIL = \u001b[33m\"\u001b[39m\u001b[33myour.email@example.com\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\__init__.py:64\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m MODULE_ATTRS:\n\u001b[32m     63\u001b[39m     module_name, attr_name = MODULE_ATTRS[name].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     module = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__package__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\entrypoints\\llm.py:20\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeVar, deprecated\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeam_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BeamSearchInstance, BeamSearchOutput,\n\u001b[32m     18\u001b[39m                               BeamSearchSequence,\n\u001b[32m     19\u001b[39m                               create_sort_beams_key_function)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (CompilationConfig, ModelDType, TokenizerMode,\n\u001b[32m     21\u001b[39m                          is_init_field)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marg_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (EngineArgs, HfOverrides, PoolerConfig,\n\u001b[32m     23\u001b[39m                                    TaskOption)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\config.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompilation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minductor_pass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallableInductorPass, InductorPass\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantizationMethods\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformers_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     ConfigFormat, get_config, get_hf_image_processor_config,\n\u001b[32m     39\u001b[39m     get_hf_text_config, get_pooling_config,\n\u001b[32m     40\u001b[39m     get_sentence_transformer_tokenizer_config, is_encoder_decoder,\n\u001b[32m     41\u001b[39m     try_get_generation_config, try_get_safetensors_metadata,\n\u001b[32m     42\u001b[39m     try_get_tokenizer_config, uses_mrope)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\model_executor\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BasevLLMParameter,\n\u001b[32m      5\u001b[39m                                            PackedvLLMParameter)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (SamplingMetadata,\n\u001b[32m      7\u001b[39m                                                    SamplingMetadataCache)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\model_executor\\parameter.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tensor_model_parallel_rank\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_executor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _make_synced_weight_loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\distributed\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommunication_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\distributed\\communication_op.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tp_group\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtensor_model_parallel_all_reduce\u001b[39m(input_: torch.Tensor) -> torch.Tensor:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\distributed\\parallel_state.py:150\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.empty(new_shape, dtype=tensor.dtype, device=tensor.device)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m supports_custom_op():\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m    151\u001b[39m     direct_register_custom_op(\n\u001b[32m    152\u001b[39m         op_name=\u001b[33m\"\u001b[39m\u001b[33mall_reduce\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    153\u001b[39m         op_func=all_reduce,\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m         dispatch_key=current_platform.dispatch_key,\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m     direct_register_custom_op(\n\u001b[32m    160\u001b[39m         op_name=\u001b[33m\"\u001b[39m\u001b[33mreduce_scatter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    161\u001b[39m         op_func=reduce_scatter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m         dispatch_key=current_platform.dispatch_key,\n\u001b[32m    165\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\platforms\\__init__.py:267\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     platform_cls_qualname = resolve_current_platform_cls_qualname()\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     _current_platform = \u001b[43mresolve_obj_by_qualname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplatform_cls_qualname\u001b[49m\u001b[43m)\u001b[49m()\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[32m    270\u001b[39m     _init_trace = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(traceback.format_stack())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\utils\\__init__.py:2539\u001b[39m, in \u001b[36mresolve_obj_by_qualname\u001b[39m\u001b[34m(qualname)\u001b[39m\n\u001b[32m   2535\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2536\u001b[39m \u001b[33;03mResolve an object by its fully qualified name.\u001b[39;00m\n\u001b[32m   2537\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2538\u001b[39m module_name, obj_name = qualname.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\vllm\\Lib\\site-packages\\vllm\\platforms\\cuda.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvs\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _C: Não foi possível encontrar o módulo especificado."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from Bio import Entrez\n",
    "from vllm import LLM\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Required by NCBI to use the Entrez API\n",
    "ENTREZ_EMAIL = \"your.email@example.com\"\n",
    "if ENTREZ_EMAIL == \"your.email@example.com\":\n",
    "    print(\"⚠️ WARNING: Please replace 'your.email@example.com' with your actual email address.\")\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "\n",
    "# The general topic to search on PubMed\n",
    "PUBMED_QUERY = \"myostatin inhibitor\"\n",
    "\n",
    "# A specific, PICO-formatted research question for the instructed embedding\n",
    "# P: Patients with Duchenne Muscular Dystrophy (DMD)\n",
    "# I: Myostatin inhibitor therapy\n",
    "# C: Placebo or standard of care\n",
    "# O: Improvement in muscle function and strength\n",
    "PICO_QUESTION = (\n",
    "    \"In patients with Duchenne Muscular Dystrophy, does myostatin inhibitor therapy, \"\n",
    "    \"compared to placebo, lead to a significant improvement in muscle function and strength?\"\n",
    ")\n",
    "\n",
    "# Model and output configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "MAX_PAPERS_TO_FETCH = 75  # Limit the number of papers for this test\n",
    "OUTPUT_CSV_FILE = \"myostatin_pico_similarity_results.csv\"\n",
    "\n",
    "# --- FUNCTIONS ---\n",
    "\n",
    "def fetch_pubmed_data(query: str, max_count: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetches article data (PMID, title, abstract) from PubMed for a given query.\n",
    "    \"\"\"\n",
    "    print(f\"Searching PubMed for: '{query}' (limit: {max_count} articles)...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Search PubMed to get a list of article IDs (PMIDs)\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_count)\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        pmids = record[\"IdList\"]\n",
    "\n",
    "        if not pmids:\n",
    "            print(\"No articles found for the query.\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Found {len(pmids)} PMIDs. Fetching details...\")\n",
    "\n",
    "        # Step 2: Fetch the detailed records for the retrieved PMIDs\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"xml\")\n",
    "        records = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        articles = []\n",
    "        for pubmed_article in records.get('PubmedArticle', []):\n",
    "            medline_citation = pubmed_article.get('MedlineCitation', {})\n",
    "            article_info = medline_citation.get('Article', {})\n",
    "            \n",
    "            # Extract abstract, handling cases where it might be missing or structured\n",
    "            abstract_parts = article_info.get('Abstract', {}).get('AbstractText', [])\n",
    "            abstract = ' '.join(part for part in abstract_parts)\n",
    "            \n",
    "            if abstract:  # Only include articles that have an abstract\n",
    "                articles.append({\n",
    "                    \"pmid\": str(medline_citation.get('PMID', '')),\n",
    "                    \"title\": article_info.get('ArticleTitle', 'No Title Found'),\n",
    "                    \"abstract\": abstract\n",
    "                })\n",
    "        \n",
    "        print(f\"Successfully fetched {len(articles)} articles with abstracts.\")\n",
    "        return articles\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data from PubMed: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_pico_instruction(pico_question: str) -> str:\n",
    "    \"\"\"Formats the PICO question into the instruction format for the model.\"\"\"\n",
    "    return (\n",
    "        \"Instruct: Find medical abstracts that address the following clinical research question.\\n\"\n",
    "        f\"Query: {pico_question}\"\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
    "    \n",
    "    # 1. Fetch data from PubMed\n",
    "    articles = fetch_pubmed_data(PUBMED_QUERY, MAX_PAPERS_TO_FETCH)\n",
    "    if not articles:\n",
    "        return\n",
    "\n",
    "    # 2. Initialize the vLLM embedding model\n",
    "    print(f\"\\nInitializing embedding model: {MODEL_NAME}...\")\n",
    "    # Use 'tensor_parallel_size' to leverage multiple GPUs if available\n",
    "    # llm = LLM(model=MODEL_NAME, task=\"embed\", tensor_parallel_size=torch.cuda.device_count())\n",
    "    llm = LLM(model=MODEL_NAME, task=\"embed\")\n",
    "\n",
    "\n",
    "    # 3. Prepare the texts for embedding\n",
    "    instructed_query = create_pico_instruction(PICO_QUESTION)\n",
    "    abstracts = [article[\"abstract\"] for article in articles]\n",
    "    \n",
    "    # The first item in the list is our query, the rest are documents\n",
    "    texts_to_embed = [instructed_query] + abstracts\n",
    "\n",
    "    # 4. Generate embeddings\n",
    "    print(f\"Generating embeddings for {len(texts_to_embed)} texts...\")\n",
    "    start_time = time.time()\n",
    "    outputs = llm.embed(texts_to_embed)\n",
    "    end_time = time.time()\n",
    "    print(f\"Embedding generation took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # 5. Calculate similarity scores\n",
    "    all_embeddings = torch.tensor([o.outputs.embedding for o in outputs])\n",
    "\n",
    "    # Best practice: L2 normalize the embeddings for accurate cosine similarity\n",
    "    all_embeddings = F.normalize(all_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Separate the query embedding from the document embeddings\n",
    "    query_embedding = all_embeddings[0]\n",
    "    document_embeddings = all_embeddings[1:]\n",
    "\n",
    "    # Calculate cosine similarity (dot product of normalized vectors)\n",
    "    similarity_scores = query_embedding @ document_embeddings.T\n",
    "    \n",
    "    # 6. Combine scores with article data\n",
    "    for i, article in enumerate(articles):\n",
    "        article['similarity_score'] = similarity_scores[i].item()\n",
    "\n",
    "    # 7. Sort articles by similarity score in descending order\n",
    "    sorted_articles = sorted(articles, key=lambda x: x['similarity_score'], reverse=True)\n",
    "\n",
    "    # 8. Save results to CSV\n",
    "    print(f\"\\nSaving results to '{OUTPUT_CSV_FILE}'...\")\n",
    "    with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['pmid', 'similarity_score', 'title', 'abstract']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for article in sorted_articles:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    print(\"\\n✅ Success! Process complete.\")\n",
    "    print(\"\\nTop 5 most relevant articles based on the PICO question:\")\n",
    "    for i, article in enumerate(sorted_articles[:5]):\n",
    "        print(f\"{i+1}. PMID: {article['pmid']} | Score: {article['similarity_score']:.4f} | Title: {article['title']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78002d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:37:18) [MSC v.1943 64 bit (AMD64)]\n",
      "Torch: 2.6.0+cu124 CUDA: 12.4\n",
      "CUDA available: True\n",
      "cudart64_124.dll  ✗ Could not find module 'cudart64_124.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "torch_cuda.dll  ✓ found\n"
     ]
    }
   ],
   "source": [
    "import os, ctypes, torch, importlib.util, sys\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "for dll in [\"cudart64_124.dll\", \"torch_cuda.dll\"]:\n",
    "    try:\n",
    "        ctypes.WinDLL(dll)\n",
    "        print(dll, \" ✓ found\")\n",
    "    except OSError as e:\n",
    "        print(dll, \" ✗\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62124e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find PubMed abstracts that best answer a PICO question\n",
    "— WITHOUT vLLM —\n",
    "Uses Qwen/Qwen3-Embedding-0.6B via Sentence-Transformers.\n",
    "\"\"\"\n",
    "\n",
    "import csv, time, torch, torch.nn.functional as F\n",
    "from typing import List, Dict, Any\n",
    "from Bio import Entrez\n",
    "from sentence_transformers import SentenceTransformer           # <-- NEW\n",
    "\n",
    "# ---------- USER CONFIG -----------------------------------------------------\n",
    "ENTREZ_EMAIL      = \"levi@example.com\"              # REQUIRED by NCBI\n",
    "PUBMED_QUERY      = \"myostatin inhibitor\"\n",
    "PICO_QUESTION     = (\"In patients with Duchenne Muscular Dystrophy, does \"\n",
    "                     \"myostatin inhibitor therapy, compared to placebo, \"\n",
    "                     \"lead to significant improvement in muscle function and strength?\")\n",
    "MAX_PAPERS        = 75\n",
    "MODEL_NAME        = \"Qwen/Qwen3-Embedding-0.6B\"     # can swap for any ST model\n",
    "USE_FP16          = True                            # set False if on CPU\n",
    "OUT_CSV           = \"myostatin_pico_similarity_results.csv\"\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "\n",
    "\n",
    "# -------------------- 1. Fetch PubMed data ---------------------------------\n",
    "def fetch_pubmed(query: str, limit: int) -> List[Dict[str, Any]]:\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=limit)\n",
    "    pmids = Entrez.read(handle)[\"IdList\"]; handle.close()\n",
    "    if not pmids: return []\n",
    "\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"xml\")\n",
    "    records = Entrez.read(handle); handle.close()\n",
    "\n",
    "    articles = []\n",
    "    for art in records.get(\"PubmedArticle\", []):\n",
    "        cite  = art[\"MedlineCitation\"]\n",
    "        info  = cite.get(\"Article\", {})\n",
    "        abst  = \" \".join(info.get(\"Abstract\", {}).get(\"AbstractText\", []))\n",
    "        if abst:\n",
    "            articles.append({\n",
    "                \"pmid\": cite.get(\"PMID\", \"\"),\n",
    "                \"title\": info.get(\"ArticleTitle\", \"—\"),\n",
    "                \"abstract\": abst})\n",
    "    return articles\n",
    "\n",
    "\n",
    "# -------------------- 2. Load embedding model ------------------------------\n",
    "dtype = torch.float16 if (torch.cuda.is_available() and USE_FP16) else torch.float32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(              # wraps AutoModel + pooling head\n",
    "    MODEL_NAME,\n",
    "    device=device,\n",
    "    cache_folder=\".hf_cache\",             # optional local cache\n",
    "    model_kwargs={\"torch_dtype\": dtype,   # forward precision\n",
    "                  \"device_map\": \"auto\"},  # split layers if multi-GPU\n",
    "    tokenizer_kwargs={\"padding_side\": \"left\"}  # recommended for Qwen3 :contentReference[oaicite:0]{index=0}\n",
    ")\n",
    "\n",
    "# -------------------- 3. Prepare texts -------------------------------------\n",
    "articles = fetch_pubmed(PUBMED_QUERY, MAX_PAPERS)\n",
    "assert articles, \"No PubMed abstracts found.\"\n",
    "\n",
    "instruction = (\n",
    "    \"Instruct: Find medical abstracts that address the following clinical research question.\\n\"\n",
    "    f\"Query: {PICO_QUESTION}\"\n",
    ")\n",
    "texts = [instruction] + [a[\"abstract\"] for a in articles]\n",
    "\n",
    "# -------------------- 4. Encode --------------------------------------------\n",
    "start = time.time()\n",
    "emb = model.encode(\n",
    "    texts,\n",
    "    batch_size=32,            # adjust to fit your GPU\n",
    "    convert_to_tensor=True,   # returns a single torch.Tensor\n",
    "    normalize_embeddings=True # built-in L2 normalisation\n",
    ")\n",
    "print(f\"Embedding time: {time.time() - start:.1f}s\")\n",
    "\n",
    "query_vec, doc_vecs = emb[0], emb[1:]\n",
    "sims = torch.matmul(doc_vecs, query_vec)   # cosine because already unit-norm\n",
    "\n",
    "# -------------------- 5. Rank & export -------------------------------------\n",
    "for art, score in zip(articles, sims):\n",
    "    art[\"similarity_score\"] = score.item()\n",
    "\n",
    "articles.sort(key=lambda d: d[\"similarity_score\"], reverse=True)\n",
    "\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, [\"pmid\", \"similarity_score\", \"title\", \"abstract\"])\n",
    "    writer.writeheader(); writer.writerows(articles)\n",
    "\n",
    "print(\"\\nTop-5 hits:\")\n",
    "for i, art in enumerate(articles[:5], 1):\n",
    "    print(f\"{i}. PMID {art['pmid']}  ({art['similarity_score']:.4f})  {art['title'][:90]}…\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
